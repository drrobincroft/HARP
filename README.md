# HARP

https://user-images.githubusercontent.com/49083808/192747399-3b3bf088-9f43-4c96-b368-a8613608110f.mp4


An Angle-Oriented Approach to Transferring Speech to Gesture for Highly Anthropomorphized Embodied Conversational Agents

This repository gives all necessary code and data link to build and train HARP, a novel neural network to trasfer speech audio into co-speech gestures.
The code and data link serve as a supplyment to the article "HARP: An Improved Approach to Transferring Speech To Gesture Based on CNN and LSTM". Anyone can download and use it.
It contains C and MATLAB code to transfer key point coordinates into angles for convinence to process, and Python Code for building and training HARP.

Abstract:
Realistic co-speech gestures are important to anthropomorphize embodied conversational agents, as nonverbal behaviour improves expressiveness of their speech greatly. However, co-speech gesture synthesis of ECAs faces a severe challenge to the domain of artificial intelligence. The existing approaches to generating co-speech gestures with sufficient details (including fingers, etc.) in 3D scenarios are indeed rare. Additionally, they hardly address the problem of abnormal gestures, temporal-spatial coherence and diversity of gesture sequences comprehensively. To address these issues, we propose an angle-based approach to transferring speech to gesture for highly anthropomorphized anthropomorphize embodied conversational agents. In detail, (1) we put forward an angle conversion method to transfer coordinates of human upper body key points into relative deflection angles and pitch angles, in order to remove body part length from the original in-the-wild video dataset and thus avoid abnormal gestures. (2) We also propose a neural network called HARP with encoder-decoder architecture to transfer MFCC featured speech audio into aforementioned angles on the basis of CNN and LSTM. The angles can be rendered as co-speech gestures via inverse process of above method. Comparing with the other latest approaches, the co-speech gestures generated by HARP are proved to be highly persuasive and credible with the stronge temporal-spatial coherence and diversity. Moreover, our approach focuses on all key points of human upper body, which put finer control on co-speech gesture than most of existing works. It can also adopt any body part length to generate co-speech gestures, which makes HARP more feasible to industrial application.

Note:
The pre-trained models of HARP are of commercial secret, so they cannot be published.
The training data for HARP orginates from "Learning Speech-driven 3D Conversational Gestures from Video" with the data link http://gvv.mpi-inf.mpg.de/projects/3d_speech_driven_gesture/

Acknowledgement:
Thanks to the great dataset contributed by Ikhsanul Habibie et al. "Learning Speech-driven 3D Conversational Gestures from Video", which is 3D training data annotations reconstructed from more than 33 hours of in-the-wild videos of talking subjects.

Here is instruction for installing and running the program. All program is tested on Windows 10 and Nvidia GeForce RTX 3090 GPU.
Preparation:
1) Downloading dataset from http://gvv.mpi-inf.mpg.de/projects/3d_speech_driven_gesture/ partly if you do not need all subjects.
2) Downloading simple MFCC extractor by Pavan Kumar from https://github.com/dspavankumar/compute-mfcc.
3) Downloading library to read/write .npy and .npz files in C/C++ by Carl Rogers from https://github.com/rogersce/cnpy.
4) Build a project for mfcc extractor MFCC.cpp. This is very easy for all involved code is plain and simple. The code shows how to transfer a wave file (only support 16000Hz BPS) into MFCC sequence.

Generating Training Data:
1) Download Matlab NPY file reader and writer from https://github.com/kwikteam/npy-matlab. We need readNPY.m and writeNPY.m, etc.
2) Modify example code PreprocessBodyDataAfterMFCC.m according to your downloaded dataset path and speech style preference (we choose gentle style for example).
3) Run mfcc extractor to get MFCC sequence from a group of origin data (e.g., subject "ellen").
4) Run PreprocessBodyDataAfterMFCC.m to get training data.

Training HARP:
1) Install torch>=1.10.2,numpy,pylab,matplotlib into Python 3.6.8.
2) Modify paths in PrepareData.py according to your downloaded dataset path.
3) HARP has a generator and a discriminator (AntiHARP). In Main.py, GH.Train(30000,5e-7) represents training generator 30000 iterations with inital learning rate 5e-7. The rest example code is similar.
4) GH.Test() represents predicting gestures with generator on the testing data. You can also use other wav file to test.
5) You should train the generator and the discriminator alternatively until is validation loss of generator decreases no more. But sometimes, we found that is not enough for LSTM ramdomly-initialized states. In that case, you should try more iterations. This process will create a file named gen_harp.pth representing the trained neural network.

Exhibition:
1) Modify paths in GenerateAnimation.m according to your downloaded dataset path and generated gesture file path.
2) Run GenerateAnimation.m to genarate MATLAB animation and MP4 video file.

harpgen(*).mp4 are videos from real human (true) or generated gestures (false), can you tell if they are true?
