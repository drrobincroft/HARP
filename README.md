# HARP

https://user-images.githubusercontent.com/49083808/192747399-3b3bf088-9f43-4c96-b368-a8613608110f.mp4


HARP: An Improved Approach to Transferring Speech To Gesture Based on CNN and LSTM

This repository gives all necessary code and data link to build and train HARP, a novel neural network to trasfer speech audio into co-speech gestures.
The code and data link serve as a supplyment to the article "HARP: An Improved Approach to Transferring Speech To Gesture Based on CNN and LSTM". Anyone can download and use it.
It contains C and MATLAB code to transfer key point coordinates into angles for convinence to process, and Python Code for building and training HARP.

Abstract:
Realistic co-speech gestures are important to anthropomorphize Embodied Conversational Agents (ECAs), for nonverbal behaviour improves expressiveness of their speech greatly. Co-speech gesture synthesis of ECAs poses a severe challenge to the domain of artificial intelligence. The existing approaches to generating co-speech gestures with sufficient details (spin, hips, hands, etc.) in 3D scenarios are indeed rare, and additionally they hardly address the problem of abnormal gestures, temporal-spatial coherence and diversity of gesture sequences comprehensively. To improve this, we propose an approach to transferring speech to gesture represented by deflection angles and pitch angles of upper body key points in consideration of speaking style. In detail, we put forward a data process method to transfer coordinates and angles of key points into each other, as well as a neural network with encoder-decoder architecture called HARP to transfer MFCC featured speech audio into aforementioned angles on the basis of CNN and LSTM. The former data process method is for removing body part length from origin dataset to avoid abnormal gestures which have changed limp length. To distinguish different speaking styles, the training data, along with the trained neuron weights, is categorized into groups according to data source. Through evaluation, the co-speech gestures generated by HARP are proved to be persuasive and credible with strong temporal-spatial coherence and diversity and without abnormal gestures, which is at the same level of the existing works. Contrast to the latter, our approach focuses on finer control on human upper body without any input except speech audio. Moreover, it can adopt any body part length to generate co-speech gestures, which is more adaptive to industrial application.

Note:
The pre-trained models of HARP are of commercial secret, so they cannot be published.
The training data for HARP orginates from "Learning Speech-driven 3D Conversational Gestures from Video" with the data link http://gvv.mpi-inf.mpg.de/projects/3d_speech_driven_gesture/

Acknowledgement:
Thanks to the great dataset contributed by Ikhsanul Habibie et al. "Learning Speech-driven 3D Conversational Gestures from Video", which is 3D training data annotations reconstructed from more than 33 hours of in-the-wild videos of talking subjects.
